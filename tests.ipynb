{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'titalib.printer' from '/home/marcel/progs/python_scripts/titanic/titalib/printer.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import importlib\n",
    "import math\n",
    "\n",
    "import titalib.preproc as prep\n",
    "import titalib.printer as prt\n",
    "import titalib.models as mdl\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "\n",
    "importlib.reload(prep)\n",
    "importlib.reload(prt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and look at the first 10 entries as comparison point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datatrain_raw = prep.dataload('data/train.csv')\n",
    "datatest_raw = prep.dataload('data/test.csv')\n",
    "datatrain = datatrain_raw.copy()\n",
    "datatest = datatest_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare it with the pre-processed version of the data using the *preproc.dataformat()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in the features:\n",
      " ['Age', 'Cabin', 'Embarked', 'Fare']\n",
      "\n",
      "'Age' categories: 2 (childs under 15)\n",
      "'Fare' categories: 3 (delimiters: 10 and 50)\n",
      "'Sex' categories: 2\n",
      "'Relatives' categories: 2\n",
      "'Location' categories: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Relatives</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age  Fare  Sex  Relatives  Location\n",
       "PassengerId                                     \n",
       "1              1     0    1          1         0\n",
       "2              1     2    0          1         3\n",
       "3              1     0    0          0         0\n",
       "4              1     2    0          1         3\n",
       "5              1     0    1          0         0\n",
       "6              1     0    1          0         0\n",
       "7              1     2    1          0         3\n",
       "8              0     0    1          0         0\n",
       "9              1     0    0          1         0\n",
       "10             0     1    0          1         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit of some categorical features\n",
    "limits = {\"child\": 15,\n",
    "          \"sibsplimit\": 2,\n",
    "          \"parchlimit\": 1,\n",
    "          \"farebound1\": 10,\n",
    "          \"farebound2\": 50,\n",
    "          \"titlelimit\": 10}\n",
    "droplist = ['Name','Ticket','Survived','Cabin','Embarked','Pclass','SibSp','Parch']\n",
    "fullset, trainsize, labels = prep.dataformat(datatrain_raw,\n",
    "                                             datatest_raw,\n",
    "                                             droplist=droplist,\n",
    "                                             limits=limits)\n",
    "fullset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. Using the *preproc.dataformat()* function, we:\n",
    "- extract labels from the **Survived** feature.\n",
    "- dropped **Name**, **Ticket** and **Survived** features.\n",
    "- replaced the 'NaN' by easily distinguishable dummy values ('Z' for **Cabin** and **Embarked**, '200' for **Age**, '0' for **Fare**).\n",
    "- normalized the **Fare** feature by the number of **Cabin** booked, and split it into 3 features defined by 2 customizable boundaries (below the first, between the 2, and above the highest)\n",
    "- turn the **Age** feature into a class feature by assigning each passenger a class label given his slice of age, which is a user-defined parameter.\n",
    "- reduced the **Cabin** feature to a single letter and move the too low populated categories into a 'garbage' category to reduce the number of parameters.\n",
    "- converts all the class features into a numeric (integer) class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now perform a one-hot encoding on all our class features (all of them currently) before being able to use them in scikit-learn. We will use the *preprocessing.OneHotEncoder()* for this task, as it output a single array of binarized features which size depends on the sum of input features range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's convert our pandas dataframe as a 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = fullset.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a mask of features that are to be 'one-hot encoded':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "enc = preprocessing.OneHotEncoder(sparse=True)\n",
    "test = enc.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1309x28 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11781 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's ok, we transformed our $1309 \\times 8$ (full data entries x class features) matrix into a $1309 \\times 32$ sparse matrix. Each of our class feature has been split into $N$ features taking only '1' or '0' value, where $N$ is the number of category of the feature. We have here 8 class features, and the sum of all classes is 32. We thus should have 8 ones and 24 zeros per row for a total of 32 features. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "        1.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which corresponds, in this order, to:\n",
    "- **Age**=1\n",
    "- **Cabin**=6\n",
    "- **Embarked**=2\n",
    "- **Fare**=0\n",
    "- **Parch**=0\n",
    "- **Pclass**=3\n",
    "- **Sex**=1\n",
    "- **SibSp**=1\n",
    "\n",
    "To the exception of the *Cabin* field, it is our 1st passenger as we can see below. The reason the *Cabin* field give a different result is because of the current implementation, which merge cabin categories **after** the string to integer conversion. It means that the field *Cabin* can take the initial 9 + 1 (for new 'merged') = 10 category values, but only a subset of them will be represented. We thus have a subset of categories instead of 10. This will change nothing for the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Age  Cabin  Embarked  Fare  Name  Parch  Pclass  Sex  SibSp\n",
       "PassengerId                                                             \n",
       "1              1      8         2     0     0      0       3    1      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement those modifications in the *preproc.onehot()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Features: 28\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "        1.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=[True,True,True,True,True,True,True,True,True]  \n",
    "datatrain, datatest, labels = prep.dataonehot(dataset=fullset,\n",
    "                                                labels=labels,\n",
    "                                                mask=mask,\n",
    "                                                trainsize=trainsize)\n",
    "datatrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing we need to do is to rescale our features if we plan to use a Support Vector Machine, which is not scale invariant. The *preproc.datanorm()* function will scale all those features to 0 mean value, the unit variance, and then output a matrix of data ready to be used by our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30974338,  0.63320091, -0.49789473, -0.23598136, -0.26629582,\n",
       "        0.54492498, -0.35154137, -0.48204268, -0.30756234,  0.61583843,\n",
       "        0.84661857, -0.62277642, -0.40019526,  0.85053175, -0.50665528,\n",
       "       -0.4039621 , -0.21680296, -0.1767767 ,  0.56049915, -0.56049915,\n",
       "       -0.56568542, -0.51015154,  0.90258736, -0.73769513,  0.73769513,\n",
       "       -1.46574551,  1.80642129, -0.30095727])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain = prep.datanorm(datatrain)\n",
    "datatest = prep.datanorm(datatest)\n",
    "datatrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start dealing with the real work, the Machine Learning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a Support Vector Machine from scikit-learn to make sure everything can run, before going deeper into any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=1000, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = datatrain\n",
    "y = labels\n",
    "classif = svm.SVC(cache_size=1000)\n",
    "classif.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85970819304152635"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working. Let's dig things deeper now. The SVM is an appropriate choice for this kind of problem (classification) but not the only solution. Given the fact we have a small amount of data entries (less than 1000 here) we will stick at it, as it is optimal conditions for it. \n",
    "\n",
    "We should try to:\n",
    "- fine tune its parameters using a grid search. We will then need to split the data into a training and a cross-validation set.\n",
    "- try different slice of *Ages*.\n",
    "- try different bounds for the 3 *Fare* categories we defined earlier.\n",
    "- try different number of categories for *SibSp* and *Parch*.\n",
    "- try a different limit for the number of *Cabin* categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# shuffle and splits the data into n_folds sets of size folds_size of the original data set\n",
    "n_folds = 5\n",
    "folds_size = 0.4\n",
    "shuffle = ShuffleSplit(n_splits=n_folds, test_size=folds_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.817\n",
      "Best parameters set:\n",
      "\tC: 4.8888888888888893\n",
      "\tgamma: 0.0080000000000000002\n",
      "\tkernel: 'rbf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "classif = svm.SVC(cache_size=1000)\n",
    "# parameters to be tested\n",
    "param_C = np.linspace(1,20,5)\n",
    "param_Gam = np.linspace(0.001,0.1,5)\n",
    "parameters = [{'C': param_C, 'gamma': param_Gam, 'kernel': ['rbf']}]\n",
    "    \n",
    "# grid search on the parameters\n",
    "grid_search = GridSearchCV(classif, parameters, cv=shuffle.split(X, y), n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# display best results\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in grid_search.best_params_.keys():\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score result is here a mean over the results from the test part of the 5 folds declared above, the model being trained on the remaining part (60% in the example above). By tuning the parameters with a finer grid search and additional feature engineering, one can reach a score of about 85%.\n",
    "\n",
    "Given the amount of data we currently have, I think it is also better to train on something about 80% of the data and test it on the remaining 20%. In order to get a reliable results, one should increase the number of folds to something like 10 or 15. By training on 50% of the data or less, our model clearly overfits too much as I could obtain a training score above 90%, but a test score around 81%. Increasing the training set size will see the training and test accuracy converge to a value between 85 and 87% with fined-tuned parameters/features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions and write the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatrain_raw = prep.dataload('data/train.csv')\n",
    "datatest_raw = prep.dataload('data/test.csv')\n",
    "datatrain = datatrain_raw.copy()\n",
    "datatest = datatest_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in the features:\n",
      "\tAge: True\n",
      "\tCabin: True\n",
      "\tEmbarked: True\n",
      "\tFare: True\n",
      "\tName: False\n",
      "\tParch: False\n",
      "\tPclass: False\n",
      "\tSex: False\n",
      "\tSibSp: False\n",
      "\tSurvived: False\n",
      "\tTicket: False\n",
      "\n",
      "Remaining Age categories: 3  (child under 15 years)\n",
      "Remaining Cabin categories: 4 (6 merged)\n",
      "Embarked categories: 3\n",
      "Fare categories delimiters: 10 and 50 (3 categories)\n",
      "Remaining Parch categories: 2\n",
      "Pclass categories: 3\n",
      "Gender categories: 2\n",
      "Remaining SibSp categories: 3\n",
      "Remaining Title categories: 5 \n",
      "\n",
      "Total number of Features: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullset, trainsize, labels = prep.dataformat(datatrain_raw, datatest_raw, droplist=droplist, limits=limits);\n",
    "datatrain, datatest, labels = prep.dataonehot(dataset=fullset, labels=labels, trainsize=trainsize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatrain = prep.datanorm(datatrain)\n",
    "datatest = prep.datanorm(datatest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=8, cache_size=2000, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.008, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif = svm.SVC(C=8,gamma=0.008,kernel='rbf',cache_size=2000)\n",
    "classif.fit(datatrain, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classif.predict(datatest)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are ready to be written in the output file. We just have to write it the correct format for the Kaggle challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
